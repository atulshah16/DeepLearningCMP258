{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shah_Atul_014530243_Auto_Diff_Gradient_MNIST.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atulshah16/DeepLearningCMP258/blob/master/Graded%20Assignment%202/Shah_Atul_014530243_Auto_Diff_Gradient_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBS_y60dmmXy",
        "colab_type": "text"
      },
      "source": [
        "#Autodiff/Autogradient implementation on MNIST classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJdY83zUupFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrLQR79QJ5Io",
        "colab_type": "text"
      },
      "source": [
        "#Defining Layers, forward and backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3d7j9g7lqz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer:\n",
        "    def Forward(self):\n",
        "        raise NotImplemented\n",
        "    def Backward(self, grad):\n",
        "        raise NotImplemented\n",
        "    def __call__(self, *args):\n",
        "        return self.Forward(*args)\n",
        "\n",
        "class Sigmoid:\n",
        "    def Forward(self,x):\n",
        "        self.x = x   \n",
        "        return 1/(1+np.exp(-x))  \n",
        "    def Backward(self, grad):\n",
        "        grad_input = self.x*(1-self.x) * grad\n",
        "        return grad_input\n",
        "\n",
        "class Relu(Layer):\n",
        "    def Forward(self,x):\n",
        "        self.x = x\n",
        "        return np.maximum(np.zeros_like(x), x)      \n",
        "    def Backward(self, grad):\n",
        "        grad_input = (self.x > 0) * grad\n",
        "        return grad_input\n",
        "\n",
        "class SoftmaxCrossentropyWithLogits(Layer):\n",
        "    def Forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        exps = np.exp(x) \n",
        "        self.softmax = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "        logits = self.softmax[np.arange(x.shape[0]),y]\n",
        "        log_likelihood = -np.log(logits)\n",
        "        loss = np.sum(log_likelihood) / x.shape[0]\n",
        "        return loss\n",
        "      \n",
        "    def Backward(self, grad=True):\n",
        "        batch = self.x.shape[0]\n",
        "        grad = self.softmax\n",
        "        grad[np.arange(batch),self.y] -= 1\n",
        "        grad = grad/batch\n",
        "        return grad\n",
        "\n",
        "class MSE(Layer):\n",
        "    def Forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        return ((x - y)**2) / (self.x.shape[0]*2)\n",
        "\n",
        "    def Backward(self, grad=None):\n",
        "        return (self.x - self.y) / self.x.shape[0]\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, input, output, lr=0.0001):\n",
        "        self.A = 2*np.random.random((input, output)) - 1\n",
        "        self.b = 2*np.random.random((output)) - 1\n",
        "        self.lr = lr\n",
        "    \n",
        "    def Forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(x,self.A) + self.b\n",
        "\n",
        "    def Backward(self, grad):\n",
        "        b_grad = grad.mean(axis=0)*self.x.shape[0]\n",
        "        A_grad = np.dot(self.x.T, grad)\n",
        "        grad_input = np.dot(grad, self.A.T)\n",
        "        self.A -= A_grad * self.lr\n",
        "        self.b -= b_grad * self.lr\n",
        "        return grad_input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phKYPifgFxSP",
        "colab_type": "text"
      },
      "source": [
        "## Invoking Auto Diff Model defined above on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlcCxptFlHMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class Model(Layer):\n",
        "\n",
        "    def __init__(self, lr=0.00001):\n",
        "        self.lr = lr\n",
        "        self.layers = [\n",
        "            Linear(784,100, lr=self.lr),\n",
        "            Relu(),\n",
        "            Linear(100,200, lr=self.lr),\n",
        "            Relu(),\n",
        "            Linear(200,10, lr=self.lr)        \n",
        "        ]\n",
        "\n",
        "    def Forward(self,x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        return x\n",
        "\n",
        "    def Backward(self, grad):\n",
        "        for l in self.layers[::-1]:\n",
        "            grad = l.Backward(grad)\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "simple = transforms.Compose([transforms.ToTensor()])\n",
        "ds = MNIST('./mnist', download=True, transform=simple)\n",
        "ld = DataLoader(ds, batch_size=2, pin_memory=True, drop_last=True) \n",
        "\n",
        "mm = Model()\n",
        "loss = SoftmaxCrossentropyWithLogits()\n",
        "_loss_avg = 0 \n",
        "for e in range(7):\n",
        "    for i, (img, label) in enumerate(ld):\n",
        "        x = img.view(2,-1).numpy()\n",
        "\n",
        "        res = mm(x)\n",
        "        _loss = loss(res, label.numpy())\n",
        "        _loss_avg += _loss.mean() # running loss mean\n",
        "        grad = loss.Backward(1)\n",
        "        mm.Backward(grad)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(_loss_avg/100)\n",
        "            _loss_avg = 0\n",
        "            print('---------')\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}